services:

  download-local-llms:
    profiles: ["load-models"]
    image: curlimages/curl:8.12.1
    environment:
      - MODEL_RUNNER_BASE_URL=${MODEL_RUNNER_BASE_URL}
    volumes:
      - ./download-models.sh:/download-models.sh
    entrypoint:
      - "sh"
      - "/download-models.sh"

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - 5050:5050
    environment:
      - HISTORY_MESSAGES=${HISTORY_MESSAGES}
      - OPTION_TEMPERATURE=${OPTION_TEMPERATURE}
      - OPTION_REPEAT_LAST_N=${OPTION_REPEAT_LAST_N}
      - OPTION_REPEAT_PENALTY=${OPTION_REPEAT_PENALTY}
      - OPTION_TOP_P=${OPTION_TOP_P}
      - OPTION_TOP_K=${OPTION_TOP_K}
      - SYSTEM_INSTRUCTIONS_PATH=${SYSTEM_INSTRUCTIONS_PATH}
      - CONTENT_PATH=${CONTENT_PATH}
    volumes:
      - ./data:/app/data
      - ./docs:/app/docs
    depends_on:
      - embeddings
      - chat


  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - 9090:8502
    environment:
      - BACKEND_SERVICE_URL=${BACKEND_SERVICE_URL}
      - PAGE_TITLE=${PAGE_TITLE}
      - PAGE_HEADER=${PAGE_HEADER}
      - PAGE_ICON=${PAGE_ICON}
    depends_on:
      - backend
      - embeddings
      - chat
    #develop:
    #  watch:
    #    - action: rebuild
    #      path: ./frontend/app.py

  embeddings:
    provider:
      type: model
      options:
        model: ${LLM_EMBEDDINGS}

  chat:
    provider:
      type: model
      options:
        model: ${LLM_CHAT}
