services:

  download-local-llms:
    profiles: ["load-models"]
    image: curlimages/curl:8.6.0
    environment:
      - MODEL_RUNNER_BASE_URL=${MODEL_RUNNER_BASE_URL}
    volumes:
      - ./download-llms.sh:/download-llms.sh
    entrypoint:
      - "sh"
      - "/download-llms.sh"
      
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - 5050:5050
    environment:
      - MODEL_RUNNER_BASE_URL=${MODEL_RUNNER_BASE_URL}
      - LLM_CHAT=${LLM_CHAT}
      - LLM_EMBEDDINGS=${LLM_EMBEDDINGS}
      - HISTORY_MESSAGES=${HISTORY_MESSAGES}
      - OPTION_TEMPERATURE=${OPTION_TEMPERATURE}
      - OPTION_REPEAT_LAST_N=${OPTION_REPEAT_LAST_N}
      - OPTION_REPEAT_PENALTY=${OPTION_REPEAT_PENALTY}
      - OPTION_TOP_P=${OPTION_TOP_P}
      - OPTION_TOP_K=${OPTION_TOP_K}
      - SYSTEM_INSTRUCTIONS_PATH=${SYSTEM_INSTRUCTIONS_PATH}
      - CONTENT_PATH=${CONTENT_PATH}
    volumes:
      - ./data:/app/data
      - ./docs:/app/docs
    #depends_on:
    #  download-local-llms:
    #    condition: service_completed_successfully

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - 9090:8502
    environment:
      - BACKEND_SERVICE_URL=${BACKEND_SERVICE_URL}
      - PAGE_TITLE=${PAGE_TITLE}
      - PAGE_HEADER=${PAGE_HEADER}
      - PAGE_ICON=${PAGE_ICON}
      - LLM_CHAT=${LLM_CHAT}
      - LLM_EMBEDDINGS=${LLM_EMBEDDINGS}
    depends_on:
      - backend
    #develop:
    #  watch:
    #    - action: rebuild
    #      path: ./frontend/app.py


