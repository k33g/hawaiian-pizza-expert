services:
  download-local-llms:
    profiles: ["load-models"]
    image: curlimages/curl:8.6.0
    environment:
      - MODEL_RUNNER_BASE_URL=${MODEL_RUNNER_BASE_URL}
    volumes:
      - ./download-models.sh:/download-models.sh
    entrypoint:
      - "sh"
      - "/download-models.sh"

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - 5050:5050
    environment:
      - MODEL_RUNNER_BASE_URL=${MODEL_RUNNER_BASE_URL}
      - LLM_CHAT=${LLM_CHAT}
      - HISTORY_MESSAGES=${HISTORY_MESSAGES}
      - OPTION_TEMPERATURE=${OPTION_TEMPERATURE}
      - OPTION_REPEAT_LAST_N=${OPTION_REPEAT_LAST_N}
      - OPTION_REPEAT_PENALTY=${OPTION_REPEAT_PENALTY}
      - OPTION_TOP_P=${OPTION_TOP_P}
      - OPTION_TOP_K=${OPTION_TOP_K}
    #depends_on:
    #  download-local-llms:
    #    condition: service_completed_successfully
    #develop:
    #  watch:
    #    - action: rebuild
    #      path: ./backend/server.js

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - 9090:8502
    environment:
      - BACKEND_SERVICE_URL=${BACKEND_SERVICE_URL}
      - PAGE_TITLE=${PAGE_TITLE}
      - PAGE_HEADER=${PAGE_HEADER}
      - PAGE_ICON=${PAGE_ICON}
      - LLM_CHAT=${LLM_CHAT}
    depends_on:
      - backend
    #develop:
    #  watch:
    #    - action: rebuild
    #      path: ./frontend/app.py

